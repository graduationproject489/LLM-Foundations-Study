## **Architectures**

The **Architectures** folder contains research papers that dive into the different architectural approaches, advancements, and design choices in Large Language Models (LLMs). These studies explore everything from foundational architectures like **Transformers** to more advanced concepts in **scaling**, **prefix-tuning**, and **decoder-only architectures**. If you're interested in understanding the evolving design principles that power modern LLMs, this section will provide essential readings.

---

### **Included Papers**

1. **[Attention Is All You Need](<https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf>)**  
   - This paper introduced the **Transformer Architecture**, a groundbreaking approach in deep learning that uses self-attention mechanisms instead of recurrence or convolutions to process sequences. The transformer has become the basis for many advanced LLMs such as GPT and BERT. This paper explains the core ideas behind attention mechanisms and how they revolutionized language modeling.

2. **[Prefix-Tuning: Optimizing Continuous Prompts for Generation](<https://aclanthology.org/2021.acl-long.353.pdf>)**  
   - In this study, prefix-tuning is proposed as a method for optimizing continuous prompts, providing a scalable approach for improving the generation capabilities of LLMs. By fine-tuning the pre-trained transformer models with a set of continuous input prompts, this approach achieves performance improvements in text generation tasks.

3. **[On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration](<https://arxiv.org/pdf/2307.03917>)**  
   - This paper discusses the **decoder-only architectures** for speech-to-text models and the integration of these architectures with LLMs, advancing the intersection between speech processing and language models. It provides insights into the architectureâ€™s suitability for various multimodal applications.

4. **[Comparative Study of Large Language Model Architectures on Frontier](<https://arxiv.org/pdf/2402.00691>)**  
   - This research provides a comparative analysis of the architectures of modern LLMs, comparing leading models' strengths and weaknesses across a variety of measures.
     
5. **[A Survey on Mixture of Experts](<https://arxiv.org/abs/2407.06204>)**  
   - This research provides a comperhensive analysis of the architecture of Mixture of Experts.

6. **[Scaling Laws for Neural Language Models](<https://arxiv.org/abs/2407.06204](https://arxiv.org/abs/2001.08361>)**  
   - This research provides an overview about scaling and its adapting architectures.

7. **[Self-Attention and Transformers: Driving the Evolution of Large Language Models](<https://ieeexplore.ieee.org/abstract/document/10401818>)**  
   - Investigate how self-attention and transformers have revolutionized the field of Natural Language Processing (NLP) and facilitated the development of advanced LLMs.

8. **[A Scoping Review of Large Language Models: Architecture and Applications](<https://ieeexplore.ieee.org/abstract/document/10549006>)**  
   -provide a broad overview of the different architectures of Large Language Models (LLMs) and explore their diverse applications in various domains such as NLP, healthcare, education, etc.

9. **[Decoder-Only LLMs are Better Controllers for Diffusion Models](<https://openreview.net/pdf?id=0nEEsPOT0r>)**  
   -enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models (LLMs), resulting ina simple yet effective adapter to allow the diffusion models to be effective.

10. **[ADVANCEMENTS IN TRANSFORMER ARCHITECTURES FOR LARGE LANGUAGE MODEL: FROM BERT TO GPT-3 AND BEYOND](<https://www.researchgate.net/profile/Deepak-Kumar-429/publication/380530250_ADVANCEMENTS_IN_TRANSFORMER_ARCHITECTURES_FOR_LARGE_LANGUAGE_MODEL_FROM_BERT_TO_GPT-3_AND_BEYOND/links/66411aa906ea3d0b74612dba/ADVANCEMENTS-IN-TRANSFORMER-ARCHITECTURES-FOR-LARGE-LANGUAGE-MODEL-FROM-BERT-TO-GPT-3-AND-BEYOND.pdf>)**  
   -This study has articulated different type of LLMs, their uses, issues and future
